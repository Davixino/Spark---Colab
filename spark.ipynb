{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davixino/Spark---Colab/blob/main/spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQJfVMk-ZHQi"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless --quiet > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "eP4NULvheXSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "PnfqR7ozeeeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"  \n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n"
      ],
      "metadata": {
        "id": "TluErz8Rehn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet findspark"
      ],
      "metadata": {
        "id": "gjK6YYNCesIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "h3Jv6g3pexyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"Ejemplo interactivo\")"
      ],
      "metadata": {
        "id": "gDmOdavJe1CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "miRDD = sc.parallelize([\"uno\", \"dos\", \"tres\",\n",
        " \"cuatro\", \"cinco\", \"seis\",\n",
        "\"siete\", \"ocho\", \"nueve\"])\n",
        "\n",
        "print(\"\\nmiRDD:\", miRDD.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag9VSe-7fgbc",
        "outputId": "66df163a-7e33-4d7b-e0bd-d68824d41b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "miRDD: ['uno', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$SPARK_HOME/bin/spark-submit /content/miPrograma.py /content/prueba.txt /content/miSalida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRMxY-ux1TkT",
        "outputId": "9cbb7350-0723-4e42-d4e6-243993d1f669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/01/16 15:41:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "22/01/16 15:41:58 INFO SparkContext: Running Spark version 3.0.0\n",
            "22/01/16 15:41:58 INFO ResourceUtils: ==============================================================\n",
            "22/01/16 15:41:58 INFO ResourceUtils: Resources for spark.driver:\n",
            "\n",
            "22/01/16 15:41:58 INFO ResourceUtils: ==============================================================\n",
            "22/01/16 15:41:58 INFO SparkContext: Submitted application: miWordCount\n",
            "22/01/16 15:41:59 INFO SecurityManager: Changing view acls to: root\n",
            "22/01/16 15:41:59 INFO SecurityManager: Changing modify acls to: root\n",
            "22/01/16 15:41:59 INFO SecurityManager: Changing view acls groups to: \n",
            "22/01/16 15:41:59 INFO SecurityManager: Changing modify acls groups to: \n",
            "22/01/16 15:41:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "22/01/16 15:41:59 INFO Utils: Successfully started service 'sparkDriver' on port 43883.\n",
            "22/01/16 15:41:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "22/01/16 15:41:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "22/01/16 15:41:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "22/01/16 15:41:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "22/01/16 15:41:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "22/01/16 15:41:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bbf5317d-0a40-4a72-a336-35fd56e6ade7\n",
            "22/01/16 15:41:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "22/01/16 15:41:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "22/01/16 15:41:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "22/01/16 15:41:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "22/01/16 15:41:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://59fe441cee8b:4041\n",
            "22/01/16 15:42:00 INFO Executor: Starting executor ID driver on host 59fe441cee8b\n",
            "22/01/16 15:42:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46625.\n",
            "22/01/16 15:42:00 INFO NettyBlockTransferService: Server created on 59fe441cee8b:46625\n",
            "22/01/16 15:42:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "22/01/16 15:42:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 59fe441cee8b, 46625, None)\n",
            "22/01/16 15:42:00 INFO BlockManagerMasterEndpoint: Registering block manager 59fe441cee8b:46625 with 366.3 MiB RAM, BlockManagerId(driver, 59fe441cee8b, 46625, None)\n",
            "22/01/16 15:42:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 59fe441cee8b, 46625, None)\n",
            "22/01/16 15:42:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 59fe441cee8b, 46625, None)\n",
            "22/01/16 15:42:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/content/spark-warehouse').\n",
            "22/01/16 15:42:01 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "22/01/16 15:42:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 294.0 KiB, free 366.0 MiB)\n",
            "22/01/16 15:42:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 366.0 MiB)\n",
            "22/01/16 15:42:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 59fe441cee8b:46625 (size: 27.0 KiB, free: 366.3 MiB)\n",
            "22/01/16 15:42:02 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "22/01/16 15:42:02 INFO FileInputFormat: Total input files to process : 1\n",
            "22/01/16 15:42:02 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "22/01/16 15:42:02 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "22/01/16 15:42:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "22/01/16 15:42:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "22/01/16 15:42:03 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\n",
            "22/01/16 15:42:03 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/miPrograma.py:23) as input to shuffle 0\n",
            "22/01/16 15:42:03 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\n",
            "22/01/16 15:42:03 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)\n",
            "22/01/16 15:42:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "22/01/16 15:42:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "22/01/16 15:42:03 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/miPrograma.py:23), which has no missing parents\n",
            "22/01/16 15:42:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.5 KiB, free 366.0 MiB)\n",
            "22/01/16 15:42:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 366.0 MiB)\n",
            "22/01/16 15:42:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 59fe441cee8b:46625 (size: 7.0 KiB, free: 366.3 MiB)\n",
            "22/01/16 15:42:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200\n",
            "22/01/16 15:42:03 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/miPrograma.py:23) (first 15 tasks are for partitions Vector(0, 1))\n",
            "22/01/16 15:42:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
            "22/01/16 15:42:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 59fe441cee8b, executor driver, partition 0, PROCESS_LOCAL, 7354 bytes)\n",
            "22/01/16 15:42:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 59fe441cee8b, executor driver, partition 1, PROCESS_LOCAL, 7354 bytes)\n",
            "22/01/16 15:42:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "22/01/16 15:42:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "22/01/16 15:42:04 INFO HadoopRDD: Input split: file:/content/prueba.txt:0+24\n",
            "22/01/16 15:42:04 INFO HadoopRDD: Input split: file:/content/prueba.txt:24+25\n",
            "22/01/16 15:42:05 INFO PythonRunner: Times: total = 752, boot = 697, init = 54, finish = 1\n",
            "22/01/16 15:42:05 INFO PythonRunner: Times: total = 748, boot = 702, init = 45, finish = 1\n",
            "22/01/16 15:42:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1850 bytes result sent to driver\n",
            "22/01/16 15:42:05 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1850 bytes result sent to driver\n",
            "22/01/16 15:42:05 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1939 ms on 59fe441cee8b (executor driver) (1/2)\n",
            "22/01/16 15:42:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1973 ms on 59fe441cee8b (executor driver) (2/2)\n",
            "22/01/16 15:42:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "22/01/16 15:42:05 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39801\n",
            "22/01/16 15:42:05 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/miPrograma.py:23) finished in 2.215 s\n",
            "22/01/16 15:42:05 INFO DAGScheduler: looking for newly runnable stages\n",
            "22/01/16 15:42:05 INFO DAGScheduler: running: Set()\n",
            "22/01/16 15:42:05 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "22/01/16 15:42:05 INFO DAGScheduler: failed: Set()\n",
            "22/01/16 15:42:05 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "22/01/16 15:42:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 90.3 KiB, free 365.9 MiB)\n",
            "22/01/16 15:42:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.5 KiB, free 365.8 MiB)\n",
            "22/01/16 15:42:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 59fe441cee8b:46625 (size: 33.5 KiB, free: 366.2 MiB)\n",
            "22/01/16 15:42:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200\n",
            "22/01/16 15:42:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "22/01/16 15:42:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n",
            "22/01/16 15:42:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 59fe441cee8b, executor driver, partition 0, NODE_LOCAL, 7143 bytes)\n",
            "22/01/16 15:42:05 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 59fe441cee8b, executor driver, partition 1, NODE_LOCAL, 7143 bytes)\n",
            "22/01/16 15:42:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "22/01/16 15:42:05 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "22/01/16 15:42:05 INFO ShuffleBlockFetcherIterator: Getting 2 (205.0 B) non-empty blocks including 2 (205.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\n",
            "22/01/16 15:42:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "22/01/16 15:42:05 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\n",
            "22/01/16 15:42:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms\n",
            "22/01/16 15:42:05 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "22/01/16 15:42:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "22/01/16 15:42:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "22/01/16 15:42:05 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "22/01/16 15:42:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "22/01/16 15:42:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "22/01/16 15:42:05 INFO PythonRunner: Times: total = 22, boot = -706, init = 727, finish = 1\n",
            "22/01/16 15:42:05 INFO PythonRunner: Times: total = 30, boot = -744, init = 773, finish = 1\n",
            "22/01/16 15:42:05 INFO FileOutputCommitter: Saved output of task 'attempt_20220116154202_0008_m_000000_0' to file:/content/miSalida\n",
            "22/01/16 15:42:05 INFO SparkHadoopMapRedUtil: attempt_20220116154202_0008_m_000000_0: Committed\n",
            "22/01/16 15:42:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2147 bytes result sent to driver\n",
            "22/01/16 15:42:05 INFO FileOutputCommitter: Saved output of task 'attempt_20220116154202_0008_m_000001_0' to file:/content/miSalida\n",
            "22/01/16 15:42:05 INFO SparkHadoopMapRedUtil: attempt_20220116154202_0008_m_000001_0: Committed\n",
            "22/01/16 15:42:05 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2104 bytes result sent to driver\n",
            "22/01/16 15:42:05 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 304 ms on 59fe441cee8b (executor driver) (1/2)\n",
            "22/01/16 15:42:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 309 ms on 59fe441cee8b (executor driver) (2/2)\n",
            "22/01/16 15:42:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "22/01/16 15:42:05 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 0.345 s\n",
            "22/01/16 15:42:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "22/01/16 15:42:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "22/01/16 15:42:05 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 2.709104 s\n",
            "22/01/16 15:42:05 INFO SparkHadoopWriter: Job job_20220116154202_0008 committed.\n",
            "22/01/16 15:42:05 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "22/01/16 15:42:05 INFO SparkUI: Stopped Spark web UI at http://59fe441cee8b:4041\n",
            "22/01/16 15:42:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "22/01/16 15:42:05 INFO MemoryStore: MemoryStore cleared\n",
            "22/01/16 15:42:05 INFO BlockManager: BlockManager stopped\n",
            "22/01/16 15:42:05 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "22/01/16 15:42:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "22/01/16 15:42:05 INFO SparkContext: Successfully stopped SparkContext\n",
            "22/01/16 15:42:05 INFO ShutdownHookManager: Shutdown hook called\n",
            "22/01/16 15:42:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d4745a0-f0de-4ad7-aeb4-3f4469aca44b\n",
            "22/01/16 15:42:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d4745a0-f0de-4ad7-aeb4-3f4469aca44b/pyspark-5c1ca522-be2c-413a-ab54-503147d2c677\n",
            "22/01/16 15:42:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-264d0d42-177d-4d6c-88cc-dfee33b46f7d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head miSalida/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwQN7ldufi0W",
        "outputId": "af82bc1a-c37c-4855-8de0-675487c30dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> miSalida/part-00000 <==\n",
            "('mineria', 1)\n",
            "('datos', 1)\n",
            "\n",
            "==> miSalida/part-00001 <==\n",
            "('hola,', 1)\n",
            "('esta', 1)\n",
            "('es', 1)\n",
            "('una', 1)\n",
            "('prueba.', 1)\n",
            "('de', 1)\n",
            "('lala', 1)\n",
            "\n",
            "==> miSalida/_SUCCESS <==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhjUgKmzB4vm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}